# ğŸŒŸ Arabic NLP Classification CLI Tool ğŸŒŸ

<div align="center">

![NLP](https://img.shields.io/badge/NLP-Arabic%20Text-blue?style=for-the-badge&logo=language)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.8+-orange?style=for-the-badge&logo=python)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

### ğŸš€ A Comprehensive Arabic Natural Language Processing Pipeline

*From Raw Text to Smart Classification - All in One Powerful CLI Tool!*

</div>

---

## ğŸ“‹ Table of Contents

- [ğŸ¯ Project Overview](#-project-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Prerequisites & Installation](#-prerequisites--installation)
- [ğŸ® Quick Start Guide](#-quick-start-guide)
- [ğŸ“š Detailed Command Guide](#-detailed-command-guide)
- [ğŸ’¡ Complete Workflow Example](#-complete-workflow-example)
- [ğŸ“Š Output Files & Visualizations](#-output-files--visualizations)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ‘¨â€ğŸ’» Project Architecture](#-project-architecture)

---

## ğŸ¯ Project Overview

This project is a **comprehensive Arabic NLP classification pipeline** that takes raw Arabic text data and transforms it into trained machine learning models. It's designed to handle the unique challenges of Arabic language processing, including:

âœ… **Diacritics removal** (Tashkeel)
âœ… **Stop word removal** with Arabic-specific stopwords
âœ… **URL and special character filtering**
âœ… **Text normalization** and cleaning
âœ… **Multiple embedding strategies** (TF-IDF, Model2Vec)
âœ… **Machine learning classification** with multiple algorithms
âœ… **Comprehensive reporting** and visualizations

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ” **Exploratory Data Analysis (EDA)** | Detailed statistical analysis and visualizations of your dataset |
| ğŸ§¹ **Text Preprocessing** | Arabic-specific text cleaning and normalization |
| ğŸ¯ **Text Embedding** | Convert text to numerical vectors using TF-IDF or Model2Vec |
| ğŸ¤– **Model Training** | Train multiple ML models (Logistic Regression, SVM, Random Forest, etc.) |
| ğŸ“ˆ **Performance Reports** | Detailed metrics, confusion matrices, and ROC curves |
| ğŸ“Š **Visualizations** | Beautiful charts and graphs for data insights |

---

## ğŸ“ Project Structure

```
Project/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                          # Entry point for the CLI
â”œâ”€â”€ ğŸ“„ README.md                        # This file
â”‚
â”œâ”€â”€ ğŸ“‚ commands/                        # Command modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ eda.py                          # Exploratory Data Analysis
â”‚   â”œâ”€â”€ preprocess.py                   # Text preprocessing commands
â”‚   â”œâ”€â”€ embed.py                        # Text embedding commands
â”‚   â””â”€â”€ train.py                        # Model training commands
â”‚
â”œâ”€â”€ ğŸ“‚ data/                            # Input data files
â”‚   â”œâ”€â”€ CompanyReviews.csv              # Original dataset
â”‚   â”œâ”€â”€ cleaned.csv                     # After preprocessing
â”‚   â”œâ”€â”€ final.csv                       # Final cleaned dataset
â”‚   â”œâ”€â”€ normalized.csv                  # Normalized text
â”‚   â””â”€â”€ nostopwords.csv                 # Without stopwords
â”‚
â”œâ”€â”€ ğŸ“‚ outputs/                         # Generated outputs
â”‚   â”œâ”€â”€ embeddings/                     # Text embeddings (vectors)
â”‚   â”œâ”€â”€ models/                         # Trained ML models
â”‚   â”œâ”€â”€ reports/                        # Performance reports (JSON)
â”‚   â””â”€â”€ visualizations/                 # Generated charts & graphs
â”‚
â””â”€â”€ ğŸ“‚ utils/                           # Utility modules
    â”œâ”€â”€ data_handler.py                 # CSV loading & processing
    â”œâ”€â”€ arabic_text.py                  # Arabic-specific functions
    â”œâ”€â”€ metrics.py                      # Evaluation metrics
    â””â”€â”€ visualization.py                # Chart generation
```

---

## ğŸ”§ Prerequisites & Installation

### Step 1ï¸âƒ£: System Requirements

Ensure you have the following installed on your machine:

```bash
# Check Python version (3.8 or higher required)
python --version

# Should output: Python 3.8.x or higher
```

### Step 2ï¸âƒ£: Clone or Download the Project

```bash
# If using git
cd /Users/khaledalamro/Desktop/NLP_Project/Project

# Or navigate to the project directory
cd Project
```

### Step 3ï¸âƒ£: Install Python Dependencies

```bash
# Method 1: Using pip (recommended)
pip install -r requirements.txt

# Method 2: Using uv (faster, if installed)
uv sync

# Method 3: Manual installation (if requirements.txt unavailable)
pip install click pandas numpy scikit-learn scipy joblib matplotlib seaborn
```

**Expected packages installed:**
- `click` - CLI framework
- `pandas` - Data manipulation
- `numpy` - Numerical computing
- `scikit-learn` - Machine learning
- `scipy` - Scientific computing
- `joblib` - Model serialization
- `matplotlib` - Plotting
- `seaborn` - Advanced visualizations

### Step 4ï¸âƒ£: Verify Installation

```bash
# Navigate to project directory
cd /Users/khaledalamro/Desktop/NLP_Project/Project

# List all available commands
python main.py --help

# You should see output like:
# Usage: main.py [OPTIONS] COMMAND [ARGS]...
#
#   Arabic NLP Classification CLI Tool
#
# Options:
#   --help  Show this message and exit.
#
# Commands:
#   eda          Exploratory Data Analysis commands
#   preprocess   Text preprocessing commands
#   embed        Text embedding commands
#   train        Train ML models on embeddings
```

---

## ğŸ® Quick Start Guide

### ğŸƒ 30-Second Quick Start

```bash
# 1. Explore your data
python main.py eda distribution --csv_path data/CompanyReviews.csv --label_col rating

# 2. Clean and preprocess the text
python main.py preprocess all --csv_path data/CompanyReviews.csv --text_col review_description --output final.csv

# 3. Create text embeddings (choose one)
python main.py embed tfidf --csv_path data/final.csv --text_col review_description --max_features 5000 --output tfidf_vectors.pkl
# OR
python main.py embed model2vec --csv_path data/final.csv --text_col review_description --output model2vec_vectors.pkl

# 4. Train machine learning models
python main.py train --csv_path data/final.csv --input_col outputs/embeddings/model2vec_vectors.pkl --output_col rating --models all

# 5. Check your results!
ls outputs/models/              # Trained models
ls outputs/reports/             # Performance reports
ls outputs/visualizations/      # Charts and confusion matrices
```

---

## ğŸ“š Detailed Command Guide

### ğŸ“Š 1. Exploratory Data Analysis (EDA)

#### Purpose
Understand your dataset structure, distribution, and characteristics before processing.

#### Command Structure
```bash
python main.py eda <subcommand> [OPTIONS]
```

#### Available Subcommands

##### **1a. View Label Distribution (Pie Chart)**
```bash
python main.py eda distribution \
    --csv_path data/CompanyReviews.csv \
    --label_col rating
```

Creates a **pie chart** showing the distribution of labels in your dataset.

**Parameters:**
| Parameter | Required | Type | Description |
|-----------|----------|------|-------------|
| `--csv_path` | âœ… Yes | string | Path to your CSV file |
| `--label_col` | âœ… Yes | string | Name of the label/target column |
| `--plot_type` | âŒ Optional | string | Chart type: `pie` (default) or `bar` |

**Output:**
- ğŸ“Š PNG chart saved to `outputs/visualizations/label_distribution_<label>_pie.png`
- ğŸ¯ Summary statistics printed to console

##### **1b. View Label Distribution (Bar Chart)**
```bash
python main.py eda distribution \
    --csv_path data/CompanyReviews.csv \
    --label_col rating \
    --plot_type bar
```

Creates a **bar chart** showing the distribution of labels.

**Output:**
- ğŸ“Š PNG chart saved to `outputs/visualizations/label_distribution_<label>_bar.png`

##### **1c. Text Length Analysis (Word Count)**
```bash
python main.py eda histogram \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --unit words
```

Analyzes text length distribution in **word count** and creates a histogram.

**Parameters:**
| Parameter | Required | Type | Description |
|-----------|----------|------|-------------|
| `--csv_path` | âœ… Yes | string | Path to your CSV file |
| `--text_col` | âœ… Yes | string | Name of the text column |
| `--unit` | âŒ Optional | string | `words` (default) or `chars` for character count |

**Output:**
- ğŸ“ˆ Histogram saved to `outputs/visualizations/text_length_<unit>_hist.png`
- Statistics (mean, median, std dev) printed to console

##### **1d. Text Length Analysis (Character Count)**
```bash
python main.py eda histogram \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --unit chars
```

Analyzes text length distribution in **character count**.

**Output:**
- ğŸ“ˆ Histogram saved to `outputs/visualizations/text_length_chars_hist.png`

**Example Output:**
```
Total samples: 37885
Mean words: 7.47
Median words: 6
Std dev: 5.23
Min words: 1
Max words: 89
```

##### **1e. Remove Statistical Outliers** â­ NEW
```bash
python main.py eda remove-outliers \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --method iqr \
    --output clean_data.csv
```

Detects and removes statistical outliers based on text length using either **IQR** or **Z-Score** method.

**Parameters:**
| Parameter | Required | Type | Default | Description |
|-----------|----------|------|---------|-------------|
| `--csv_path` | âœ… Yes | string | - | Path to your CSV file |
| `--text_col` | âœ… Yes | string | - | Name of the text column |
| `--method` | âŒ Optional | string | `iqr` | Detection method: `iqr` or `zscore` |
| `--output` | âœ… Yes | string | - | Output CSV filename (saved to `data/`) |

**IQR Method (Default - Recommended):**
- Calculates Q1 (25th percentile) and Q3 (75th percentile)
- Removes texts outside: [Q1 - 1.5Ã—IQR, Q3 + 1.5Ã—IQR]
- Good for skewed distributions
- More robust to extreme outliers

**Z-Score Method:**
- Calculates mean and standard deviation
- Removes texts with |Z-Score| > 3
- Good for normally distributed data
- Removes extreme values (3+ std devs from mean)

**Output:**
```
Processing: data/CompanyReviews.csv
Text column: review_description
Method: IQR
---
Q1 (25th percentile): 5.0 words
Q3 (75th percentile): 10.0 words
IQR: 5.0 words
Lower bound: 1.0 words
Upper bound: 17.5 words
---
Original rows: 40046
Outliers detected: 1161
Rows kept: 38885
Outliers removed: 2.9%
Saved â†’ data/clean_data.csv
```

**When to Use:**
- ğŸ¯ Before preprocessing: Clean extreme outliers first
- ğŸ“Š After EDA: Identify suspicious data points
- ğŸ§¹ Before embedding: Ensure consistent text length
- âš¡ For better model training: Remove noise from data

---

### ğŸ§¹ 2. Text Preprocessing

#### Purpose
Clean and normalize Arabic text by removing diacritics, stopwords, URLs, and special characters.

#### Command Structure
```bash
python main.py preprocess <subcommand> [OPTIONS]
```

#### Available Subcommands

##### **2a. Remove Special Characters, URLs, Numbers, and Diacritics**
```bash
python main.py preprocess remove \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --output cleaned.csv
```

Removes:
- ğŸ”¤ Arabic diacritics (Tashkeel: Ù‹ ÙŒ Ù Ù Ù Ù Ù‘ Ù’)
- ğŸ”— URLs and links
- ğŸ”¢ Numbers and digits
- âœ¨ Special characters (keeping only Arabic letters)

**Parameters:**
| Parameter | Required | Type | Description |
|-----------|----------|------|-------------|
| `--csv_path` | âœ… Yes | string | Path to your CSV file |
| `--text_col` | âœ… Yes | string | Name of the text column |
| `--output` | âœ… Yes | string | Output CSV filename |

**Output:**
- ğŸ“„ Cleaned CSV file (saved to `data/<output>`)
- ğŸ“Š Console report with before/after statistics

##### **2b. Remove Stopwords**
```bash
python main.py preprocess stopwords \
    --csv_path data/cleaned.csv \
    --text_col review_description \
    --output nostopwords.csv
```

Removes common Arabic stopwords like: Ù…Ù†ØŒ ÙÙŠØŒ Ù‡Ø°Ø§ØŒ Ù‡ÙˆØŒ Ù„ÙŠØ³ØŒ Ø¥Ù„Ù‰ØŒ ÙˆØŒ Ø£ÙˆØŒ Ø£Ù†ØŒ etc.

**Output:**
- ğŸ“„ CSV without stopwords
- ğŸ“Š Word count statistics before/after

##### **2c. Normalize Arabic Text**
```bash
python main.py preprocess replace \
    --csv_path data/nostopwords.csv \
    --text_col review_description \
    --output normalized.csv
```

Normalizes Arabic text by:
- ğŸ”¤ Converting hamza variants (Ø£ØŒ Ø¥ØŒ Ø¤) â†’ Ø§
- ğŸ”¤ Converting Ø© (taa marboota) â†’ Ù‡
- ğŸ”¤ Converting Ù‰ (alef maksura) â†’ ÙŠ

**Output:**
- ğŸ“„ Normalized CSV file

##### **2d. Run All Steps at Once (Recommended!)**
```bash
python main.py preprocess all \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --output final.csv
```

**The recommended approach!** Runs the complete preprocessing pipeline in optimal order:

1. Remove special characters and URLs
2. Remove numbers and digits
3. Remove diacritics (tashkeel)
4. Remove stopwords
5. Normalize Arabic characters
6. Clean whitespace

**Output:**
```
Rows before: 40046
Rows after : 37885
Avg words before: 9.37
Avg words after : 7.47
Saved â†’ data/final.csv
```

---

### ğŸ¯ 3. Text Embedding

#### Purpose
Convert text into numerical vectors that machine learning models can understand.

#### Command Structure
```bash
python main.py embed <subcommand> [OPTIONS]
```

#### Available Subcommands

##### **3a. TF-IDF Embedding**
```bash
python main.py embed tfidf \
    --csv_path data/final.csv \
    --text_col review_description \
    --max_features 5000 \
    --output tfidf_vectors.pkl
```

Creates **TF-IDF vectors** (Term Frequency-Inverse Document Frequency) using scikit-learn.

**Parameters:**
| Parameter | Default | Description |
|-----------|---------|-------------|
| `--csv_path` | Required | Path to cleaned CSV file |
| `--text_col` | Required | Name of text column to vectorize |
| `--max_features` | 5000 | Maximum number of features to extract |
| `--ngram_range` | "1 2" | Unigrams and bigrams (1-2 word phrases) |
| `--min_df` | 2 | Minimum document frequency |
| `--max_df` | 0.8 | Maximum document frequency |
| `--output` | Required | Output filename (saved to `outputs/embeddings/`) |

**Output:**
- âœ… Trained vectorizer saved
- ğŸ“Š Embedding statistics (shape, sparsity, memory usage)
- ğŸ’¾ Vectors saved as sparse matrix (.pkl file)

**Example Output:**
```
TF-IDF vectors shape: (37885, 5000) | nnz=236867 | approx_mem=2.86 MB
Saved â†’ outputs/embeddings/tfidf_vectors.pkl
```

##### **3b. Model2Vec Embedding**
```bash
python main.py embed model2vec \
    --csv_path data/final.csv \
    --text_col review_description \
    --output model2vec_vectors.pkl
```

Uses **pre-trained Arabic word embeddings** from HuggingFace (Model2Vec-ARBERTv2).

**Output:**
```
Model2Vec vectors shape: (37885, 128) | dtype=float64 | approx_mem=37.00 MB
Saved â†’ outputs/embeddings/model2vec_vectors.pkl
```

**Why Model2Vec?**
- âœ¨ Pre-trained on large Arabic corpus
- ğŸ“‰ Lower memory footprint than TF-IDF (128 dims vs 5000)
- ğŸ¯ Semantic similarity captured
- ğŸš€ Better for small datasets

---

### ğŸ¤– 4. Model Training

#### Purpose
Train machine learning models on the embedded text to classify documents.

#### Command Structure
```bash
python main.py train [OPTIONS]
```

#### Main Command
```bash
python main.py train \
    --csv_path data/final.csv \
    --input_col outputs/embeddings/model2vec_vectors.pkl \
    --output_col rating \
    --test_size 0.2 \
    --models knn lr rf
```

**Main Parameters:**
| Parameter | Required | Type | Default | Description |
|-----------|----------|------|---------|-------------|
| `--csv_path` | âœ… | string | - | Path to CSV with data and labels |
| `--input_col` | âœ… | string | - | Path to embeddings file (.pkl) |
| `--output_col` | âœ… | string | - | Target column to predict (label column) |
| `--test_size` | âŒ | float | 0.2 | Test set percentage (20% = 0.2) |
| `--models` | âŒ | string | "knn lr rf" | Models to train (space-separated) |
| `--random_state` | âŒ | int | 42 | Random seed for reproducibility |

#### Single Model Training
```bash
python main.py train \
    --csv_path data/final.csv \
    --input_col outputs/embeddings/tfidf_vectors.pkl \
    --output_col rating \
    --models lr
```

**Available Models:**
- `knn` - K-Nearest Neighbors (fast, simple)
- `lr` - Logistic Regression (fast, interpretable)
- `rf` - Random Forest (ensemble, powerful)
- `svm` - Support Vector Machine (slower, accurate)

- `all` - Train all available models

#### Multiple Models Training
```bash
python main.py train \
    --csv_path data/final.csv \
    --input_col outputs/embeddings/model2vec_vectors.pkl \
    --output_col rating \
    --models "knn lr rf svm"
```

#### Train All Models at Once
```bash
python main.py train \
    --csv_path data/final.csv \
    --input_col outputs/embeddings/model2vec_vectors.pkl \
    --output_col rating \
    --models all
```

#### Training Output

**Console Output:**
```
[train] CSV: data/final.csv
[train] Embeddings source: outputs/embeddings/model2vec_vectors.pkl
[train] Label column: rating
[train] Loaded CSV rows=37885
[train] Loading embeddings from file: outputs/embeddings/model2vec_vectors.pkl
[train] Embeddings shape: (37885, 128)
[train] Detected classes (3): ['-1', '0', '1']
[train] Split: train=30296 test=7574 (test_size=0.2)
[train] Training model: knn
[train] Done: knn -> acc=0.7678 prec=0.5665 rec=0.5441 f1=0.5401
[train] Training model: lr
[train] Done: lr -> acc=0.7811 prec=0.5142 rec=0.5385 f1=0.5260
[train] Training model: rf
[train] Done: rf -> acc=0.7827 prec=0.6158 rec=0.5408 f1=0.5349
[train] Saved best model: outputs/models/best_model_20260117_045535.pkl
[train] Saved report: outputs/reports/training_report_20260117_045535.md

âœ… Report saved â†’ outputs/reports/training_report_20260117_045535.md
âœ… Best model saved â†’ outputs/models/best_model_20260117_045535.pkl
```

**Generated Files:**
- ğŸ“¦ `outputs/models/best_model_<timestamp>.pkl` - Best trained model
- ğŸ“„ `outputs/reports/training_report_<timestamp>.md` - Full performance report
- ğŸ“Š `outputs/visualizations/cm_<model>_<timestamp>.png` - Confusion matrices

**Report Contents:**
```markdown
# Training Report - 2026-01-17 04:55:35

## Dataset Info
- Total samples: 37,885
- Train/Test split: 30,296/7,574 (80/20)
- Classes: 3
- Features: 128

## Model Performance

### K-Nearest Neighbors
- Accuracy:  76.78%
- Precision: 56.65%
- Recall:    54.41%
- F1-Score:  54.01%

### Logistic Regression  
- Accuracy:  78.11%
- Precision: 51.42%
- Recall:    53.85%
- F1-Score:  52.60%

### Random Forest â­ (Best)
- Accuracy:  78.27%
- Precision: 61.58%
- Recall:    54.08%
- F1-Score:  53.49%

## Confusion Matrices
[PNG visualizations saved]
```

---

## ğŸ’¡ Complete Workflow Example

### ğŸ”„ Step-by-Step Tutorial: From Raw Data to Trained Models

#### **Step 1: Prepare Your Data** 
Ensure you have a CSV file with at least a text column and optional label column:

```csv
review_description,rating,company
Ù‡Ø°Ø§ Ù…Ù†ØªØ¬ Ø±Ø§Ø¦Ø¹ ÙˆÙ…Ù…ØªØ§Ø²,1,CompanyA
Ø¬ÙˆØ¯Ø© Ø³ÙŠØ¦Ø© Ø¬Ø¯Ø§Ù‹,-1,CompanyB
Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ù…ØªØ§Ø²Ø©,1,CompanyC
```

**Column names in example:**
- Text column: `review_description`
- Label column: `rating`
- Other columns: `company` (optional)

#### **Step 2: Explore the Data with EDA**

View label distribution:
```bash
python main.py eda distribution \
    --csv_path data/CompanyReviews.csv \
    --label_col rating
```

View text length statistics (word count):
```bash
python main.py eda histogram \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --unit words
```

View text length statistics (character count):
```bash
python main.py eda histogram \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --unit chars
```

*(Optional) Remove statistical outliers:*
```bash
python main.py eda remove-outliers \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --method iqr \
    --output clean_data.csv
```

ğŸ“Š **Check outputs:**
- `outputs/visualizations/label_distribution_rating_pie.png`
- `outputs/visualizations/text_length_words_hist.png`
- `outputs/visualizations/text_length_chars_hist.png`
- `data/clean_data.csv` (if using outlier removal)

#### **Step 3: Clean the Text (Preprocessing)**

**Optional Step 3a: Remove Outliers**
```bash
python main.py eda remove-outliers \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --method iqr \
    --output outliers_removed.csv
```

**Main Step 3b: Use all preprocessing steps at once (recommended):**
```bash
python main.py preprocess all \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --output final.csv
```

Or if you removed outliers first:
```bash
python main.py preprocess all \
    --csv_path data/outliers_removed.csv \
    --text_col review_description \
    --output final.csv
```

**Detailed Step-by-Step Option (Alternative):**
```bash
# Remove special chars, URLs, numbers, diacritics
python main.py preprocess remove \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --output cleaned.csv

# Remove stopwords
python main.py preprocess stopwords \
    --csv_path data/cleaned.csv \
    --text_col review_description \
    --output nostopwords.csv

# Normalize Arabic characters
python main.py preprocess replace \
    --csv_path data/nostopwords.csv \
    --text_col review_description \
    --output normalized.csv

# Run all preprocessing at once
python main.py preprocess all \
    --csv_path data/normalized.csv \
    --text_col review_description \
    --output final.csv
```

âœ… **Result:** Clean, normalized Arabic text ready for embedding

#### **Step 4: Create Embeddings**

**Option A: TF-IDF Embedding (faster, uses more dimensions)**
```bash
python main.py embed tfidf \
    --csv_path data/final.csv \
    --text_col review_description \
    --max_features 5000 \
    --output tfidf_vectors.pkl
```

**Option B: Model2Vec Embedding (pre-trained, semantic)**
```bash
python main.py embed model2vec \
    --csv_path data/final.csv \
    --text_col review_description \
    --output model2vec_vectors.pkl
```

ğŸ¯ **Result:** Text converted to numerical vectors

#### **Step 5: Train Models**

**Option A: Train specific models (KNN, Logistic Regression, Random Forest)**
```bash
python main.py train \
    --csv_path data/final.csv \
    --input_col outputs/embeddings/model2vec_vectors.pkl \
    --output_col rating \
    --models "knn lr rf" \
    --test_size 0.2
```

**Option B: Train all available models**
```bash
python main.py train \
    --csv_path data/final.csv \
    --input_col outputs/embeddings/model2vec_vectors.pkl \
    --output_col rating \
    --models all \
    --test_size 0.2
```

ğŸ¤– **Result:** Trained models + performance report + confusion matrices

#### **Step 6: Review Results**

List all trained models:
```bash
ls outputs/models/
```

List all reports:
```bash
ls outputs/reports/
```

View the latest report:
```bash
cat outputs/reports/training_report_*.md | tail -100
```

View confusion matrices:
```bash
ls outputs/visualizations/cm_*.png
```

---

## ğŸ“Š Output Files & Visualizations

### ğŸ“ Directory Structure After Running

```
outputs/
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ tfidf.joblib                    # TF-IDF vectorizer
â”‚   â”œâ”€â”€ tfidf_vectors.joblib            # TF-IDF sparse matrix
â”‚   â””â”€â”€ model2vec_vectors.pkl           # Model2Vec embeddings
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic_regression_model.pkl
â”‚   â”œâ”€â”€ logistic_regression_vectorizer.pkl
â”‚   â”œâ”€â”€ svm_model.pkl
â”‚   â”œâ”€â”€ random_forest_model.pkl
â”‚   â””â”€â”€ ... (more models)
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ training_report_20260117_045535.md  # Comprehensive report
â”‚   â”œâ”€â”€ training_report_20260117_045625.md  # Another report
â”‚   â”œâ”€â”€ eda_report_<timestamp>.json
â”‚   â””â”€â”€ embedding_report_<timestamp>.json
â”‚
â””â”€â”€ visualizations/
    â”œâ”€â”€ label_distribution.png           # Label frequency chart
    â”œâ”€â”€ text_length_distribution.png     # Text length histogram
    â”œâ”€â”€ confusion_matrix.png             # Confusion matrix heatmap
    â”œâ”€â”€ roc_curve.png                    # ROC curve
    â”œâ”€â”€ feature_importance.png           # Top features
    â””â”€â”€ model_comparison.png             # Performance comparison
```

### ğŸ“ˆ Sample Report Content

```markdown
# Training Report - 2026-01-17 04:55:35

## Configuration
- Dataset: data/final.csv
- Input: outputs/embeddings/tfidf.joblib
- Target: rating
- Models: logistic-regression
- Test Size: 20%

## Results Summary
- Accuracy: 84.7%
- Precision: 84.2%
- Recall: 84.0%
- F1-Score: 84.1%

## Confusion Matrix
```
             Negative    Positive
Negative        800          50
Positive         60         290
```

## ROC-AUC Score: 0.912

## Top 10 Features (Words)
1. Ø±Ø§Ø¦Ø¹ - 0.523
2. Ù…Ù…ØªØ§Ø² - 0.498
3. Ø¬ÙˆØ¯Ø© - 0.456
...
```

---

## ğŸ Bonus Features (Available Now! âœ¨)

### ğŸ¯ **Outlier Detection & Removal** â­

Remove statistical outliers from your dataset to improve data quality before training.

```bash
# Using IQR method (recommended)
python main.py eda remove-outliers \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --method iqr \
    --output clean_data.csv

# Using Z-Score method
python main.py eda remove-outliers \
    --csv_path data/CompanyReviews.csv \
    --text_col review_description \
    --method zscore \
    --output clean_data.csv
```

**Benefits:**
- âœ… Removes extremely short/long reviews
- âœ… Improves model training by removing noise
- âœ… Makes data more consistent
- âœ… Two robust statistical methods

**Example Workflow with Outlier Removal:**
```bash
# 1. Explore raw data
python main.py eda histogram --csv_path data/CompanyReviews.csv --text_col review_description --unit words

# 2. Remove outliers
python main.py eda remove-outliers --csv_path data/CompanyReviews.csv --text_col review_description --method iqr --output clean_data.csv

# 3. Preprocess cleaned data
python main.py preprocess all --csv_path data/clean_data.csv --text_col review_description --output final.csv

# 4. Continue with embedding & training
python main.py embed model2vec --csv_path data/final.csv --text_col review_description --output model2vec_vectors.pkl
python main.py train --csv_path data/final.csv --input_col outputs/embeddings/model2vec_vectors.pkl --output_col rating --models all
```

---

### âŒ Problem: "Command not found" or "python: command not found"

**Solution:**
```bash
# Use full path to Python
/usr/bin/python3 main.py --help

# Or verify Python is installed
which python3
```

### âŒ Problem: "ModuleNotFoundError: No module named 'click'"

**Solution:**
```bash
# Install missing dependencies
pip install click pandas numpy scikit-learn scipy

# Or reinstall all
pip install -r requirements.txt
```

### âŒ Problem: "FileNotFoundError: data/CompanyReviews.csv"

**Solution:**
```bash
# Check file exists
ls -la data/

# Use correct path (case-sensitive on Mac)
python main.py preprocess all --csv_path data/CompanyReviews.csv --text_col text --output_path data/cleaned.csv
```

### âŒ Problem: "MemoryError" with large datasets

**Solution:**
```bash
# Reduce max_features for embedding
python main.py embed tfidf \
    --csv_path data/cleaned.csv \
    --text_col text \
    --max_features 2000 \
    --output_path outputs/embeddings/tfidf.joblib
```

### âŒ Problem: "No such file or directory: outputs/embeddings/..."

**Solution:**
```bash
# Directories are created automatically, but ensure embeddings exist
python main.py embed tfidf \
    --csv_path data/cleaned.csv \
    --text_col text \
    --output_path outputs/embeddings/tfidf.joblib
```

### âŒ Problem: Model training is very slow

**Solution:**
```bash
# Use faster models first
python main.py train \
    --csv_path data/cleaned.csv \
    --input_col outputs/embeddings/tfidf.joblib \
    --output_col rating \
    --models logistic-regression  # Fast!

# Avoid these for large datasets:
# - mlp (Neural network - slowest)
# - svm (Support Vector Machine - slow)
```

---

## ğŸ‘¨â€ğŸ’» Project Architecture

### ğŸ”§ Technical Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **CLI Framework** | Click | Command-line interface |
| **Data Processing** | Pandas, NumPy | Manipulation & analysis |
| **ML Algorithms** | Scikit-learn | Model training & evaluation |
| **Text Processing** | Scikit-learn | TF-IDF vectorization |
| **Visualization** | Matplotlib, Seaborn | Charts & graphs |
| **Serialization** | Joblib, Pickle | Model persistence |

### ğŸ“¦ Module Breakdown

#### **commands/eda.py**
- Statistical analysis of datasets
- Label distribution analysis
- Text length statistics
- Visualization generation

#### **commands/preprocess.py**
- Diacritics removal (Tashkeel)
- Stopword removal (Arabic-specific)
- URL and digit removal
- Text normalization
- Character filtering

#### **commands/embed.py**
- TF-IDF vectorization
- Model2Vec embeddings
- Sparse matrix generation
- Vector serialization

#### **commands/train.py**
- Multiple ML algorithms
- Train/test splitting
- Model evaluation
- Metrics calculation
- Report generation

#### **utils/data_handler.py**
- CSV loading and validation
- Column existence checking
- Data type conversion

#### **utils/visualization.py**
- Pie charts for labels
- Histograms for text length
- Confusion matrices
- ROC curves
- Feature importance plots

#### **utils/metrics.py**
- Accuracy, Precision, Recall, F1
- Confusion matrix generation
- ROC curve calculation
- Performance comparison

---

## ï¿½ Learning Tips

### ğŸ“š Understanding the Workflow

1. **EDA First** â†’ Always explore your data before processing
2. **Preprocess Second** â†’ Clean Arabic text properly
3. **Embed Third** â†’ Convert text to numbers
4. **Train Last** â†’ Build and evaluate models

### ğŸ” Best Practices

âœ… Always save your reports
âœ… Start with small datasets to test
âœ… Compare multiple models
âœ… Keep track of your preprocessing steps
âœ… Save your best models for later use

### ğŸ“– Arabic NLP Challenges

- **Diacritics**: Same words can look different with marks
- **Stopwords**: Common words that don't add meaning
- **Morphology**: Rich word formations
- **Short vowels**: Often omitted in text

This project handles all these challenges! ğŸ‰

---
